#!/usr/bin/env python3
"""
Real-time subtitle overlayï¼ˆLinux/Windowsï¼‰ã€‚

Usage:
    python subtitle_client.py --asr-server http://<SERVER_IP>:8000 --openai-api-key sk-...

Requirements:
    pip install sounddevice numpy scipy requests openai
"""
import argparse
import multiprocessing
import os
import queue
import subprocess
import threading
import time
from abc import ABC, abstractmethod
from typing import Callable

import numpy as np
import requests
import scipy.signal as signal
import tkinter as tk
from openai import OpenAI


# ---------------------------------------------------------------------------
# ASR Client
# ---------------------------------------------------------------------------

class ASRClient:
    """HTTP client for Qwen3-ASR streaming server."""

    def __init__(self, base_url: str):
        self.base_url = base_url.rstrip("/")
        self.session_id: str | None = None

    def start(self):
        """å»ºç«‹æ–°çš„ streaming sessionã€‚"""
        r = requests.post(f"{self.base_url}/api/start", timeout=10)
        r.raise_for_status()
        self.session_id = r.json()["session_id"]

    def push_chunk(self, audio_float32: np.ndarray) -> dict:
        """
        é€å‡ºä¸€æ®µ 16kHz float32 éŸ³è¨Šï¼Œå›å‚³ {"language": str, "text": str}ã€‚
        audio_float32: shape (N,), dtype float32
        """
        assert self.session_id, "Call start() first"
        r = requests.post(
            f"{self.base_url}/api/chunk",
            params={"session_id": self.session_id},
            data=audio_float32.tobytes(),
            headers={"Content-Type": "application/octet-stream"},
            timeout=60,
        )
        r.raise_for_status()
        return r.json()

    def finish(self) -> dict:
        """çµæŸ sessionï¼Œå›å‚³æœ€çµ‚çµæœã€‚"""
        assert self.session_id, "Call start() first"
        sid = self.session_id
        self.session_id = None
        r = requests.post(
            f"{self.base_url}/api/finish",
            params={"session_id": sid},
            timeout=60,
        )
        r.raise_for_status()
        return r.json()


# ---------------------------------------------------------------------------
# Translation Debouncer
# ---------------------------------------------------------------------------

class TranslationDebouncer:
    """
    å°‡è‹±æ–‡ ASR æ–‡å­— debounce å¾Œé€ GPT-4o mini ç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚

    ä½¿ç”¨æ–¹å¼ï¼š
        def on_translation(zh_text):
            print(zh_text)

        debouncer = TranslationDebouncer(api_key="sk-...", callback=on_translation)
        debouncer.update("Hello world")  # æ¯æ¬¡ ASR æ›´æ–°æ™‚å‘¼å«
        debouncer.shutdown()
    """

    SENTENCE_ENDINGS = {".", "?", "!", "ã€‚", "ï¼Ÿ", "ï¼"}
    DEBOUNCE_SEC = 0.4

    def __init__(self, api_key: str, callback, model: str = "gpt-4o-mini"):
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.callback = callback
        self.direction: str = "enâ†’zh"   # ç›®å‰ç¿»è­¯æ–¹å‘

        self._last_translated = ""
        self._pending_text = ""
        self._timer: threading.Timer | None = None
        self._lock = threading.Lock()

    def update(self, text: str):
        """æ¯æ¬¡ ASR æ›´æ–°æ™‚å‘¼å«ã€‚text æ˜¯ç›®å‰çš„å®Œæ•´è½‰éŒ„æ–‡å­—ã€‚"""
        translate_now = None
        with self._lock:
            if text == self._pending_text:
                return
            self._pending_text = text

            # å¥å°¾ç«‹å³ç¿»è­¯ï¼ˆæ³¨æ„ï¼š_do_translate å¿…é ˆåœ¨ lock é‡‹æ”¾å¾Œå‘¼å«ï¼‰
            if text and text[-1] in self.SENTENCE_ENDINGS:
                self._cancel_timer()
                translate_now = text
            else:
                # ä¸€èˆ¬ debounce
                self._cancel_timer()
                self._timer = threading.Timer(self.DEBOUNCE_SEC, self._on_timer)
                self._timer.daemon = True
                self._timer.start()

        # lock å·²é‡‹æ”¾ï¼Œæ‰å¯å‘¼å« OpenAIï¼ˆå¦å‰‡ _do_translate å…§çš„ with self._lock æœƒæ­»é–ï¼‰
        if translate_now:
            self._do_translate(translate_now)

    def _cancel_timer(self):
        if self._timer:
            self._timer.cancel()
            self._timer = None

    def _on_timer(self):
        with self._lock:
            text = self._pending_text
        self._do_translate(text)

    def toggle_direction(self) -> str:
        """åˆ‡æ›ç¿»è­¯æ–¹å‘ï¼Œå›å‚³æ–°æ–¹å‘å­—ä¸²ã€‚"""
        with self._lock:
            self.direction = "zhâ†’en" if self.direction == "enâ†’zh" else "enâ†’zh"
            self._last_translated = ""  # æ¸…ç©ºå¿«å–ï¼Œå¼·åˆ¶é‡æ–°ç¿»è­¯
            return self.direction

    def set_direction(self, direction: str) -> None:
        """ç›´æ¥è¨­å®šæ–¹å‘ï¼ˆ'enâ†’zh' æˆ– 'zhâ†’en'ï¼‰ã€‚"""
        with self._lock:
            self.direction = direction
            self._last_translated = ""

    def _do_translate(self, text: str):
        with self._lock:
            if not text or text == self._last_translated:
                return
            self._last_translated = text
            direction = self.direction  # snapshot
        # lock é‡‹æ”¾å¾Œæ‰å‘¼å« OpenAI
        if direction == "enâ†’zh":
            system_msg = (
                "ä½ æ˜¯å³æ™‚å­—å¹•ç¿»è­¯å“¡ã€‚å°‡è‹±æ–‡èªéŸ³è½‰éŒ„ç¿»è­¯æˆè‡ªç„¶æµæš¢çš„ç¹é«”ä¸­æ–‡ï¼ˆå°ç£å£èªç”¨èªï¼‰ã€‚"
                "è¦æ±‚ï¼š\n"
                "1. ä¾ç…§ä¸­æ–‡èªæ³•é‡æ–°çµ„å¥ï¼Œä¸è¦é€å­—ç¿»è­¯æˆ–ç…§æ¬è‹±æ–‡èªåº\n"
                "2. ä½¿ç”¨å°ç£äººæ—¥å¸¸èªªè©±çš„æ–¹å¼ï¼Œå£èªè‡ªç„¶\n"
                "3. å°ˆæœ‰åè©ã€äººåã€å“ç‰Œå¯ä¿ç•™è‹±æ–‡åŸæ–‡\n"
                "4. åªè¼¸å‡ºç¿»è­¯çµæœï¼Œä¸åŠ ä»»ä½•è§£é‡‹æˆ–æ¨™æ³¨"
            )
        else:  # zhâ†’en
            system_msg = (
                "You are a real-time subtitle translator. "
                "Translate the Chinese speech transcript to natural, colloquial English. "
                "Output ONLY the translation, no explanations."
            )
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": text},
                ],
                max_tokens=200,
                temperature=0.1,
            )
            translated = response.choices[0].message.content.strip()
            self.callback(translated)
        except Exception as e:
            print(f"[Translation error] {e}")

    def shutdown(self):
        with self._lock:
            self._cancel_timer()


# ---------------------------------------------------------------------------
# Subtitle Overlay Window
# ---------------------------------------------------------------------------

class SubtitleOverlay:
    """
    Always-on-top åŠé€æ˜å­—å¹•è¦–çª—ï¼Œå›ºå®šåœ¨æŒ‡å®šè¢å¹•åº•éƒ¨ã€‚

    ä½¿ç”¨æ–¹å¼ï¼š
        overlay = SubtitleOverlay(screen_index=0)
        overlay.set_text(original="Hello world", translated="ä½ å¥½ä¸–ç•Œ")
        overlay.run()  # é˜»å¡ï¼Œåœ¨ä¸»åŸ·è¡Œç·’å‘¼å«
    """

    TOOLBAR_HEIGHT = 28
    WINDOW_HEIGHT = 148         # åŸ 120 + TOOLBAR_HEIGHT
    TOOLBAR_BG = "#1a1a1a"
    BTN_COLOR = "#ffffff"
    BTN_BG = "#333333"
    BG_COLOR = "#000000"
    EN_COLOR = "#888888"
    ZH_COLOR = "#ffffff"
    EN_FONT = ("Arial", 14)
    ZH_FONT = ("Microsoft JhengHei", 22, "bold")  # Windows ç¹ä¸­å­—é«”

    def __init__(self, screen_index: int = 0, on_toggle_direction=None, on_switch_source=None):
        self._on_toggle_direction = on_toggle_direction
        self._on_switch_source = on_switch_source

        self._root = tk.Tk()

        # ç”¨ tkinter å–è¢å¹•å°ºå¯¸ï¼ˆä¸ä¾è³´ screeninfoï¼‰
        self._width = self._root.winfo_screenwidth()
        screen_height = self._root.winfo_screenheight()
        self._x = 0
        self._y = screen_height - self.WINDOW_HEIGHT

        self._root.overrideredirect(True)
        self._root.wm_attributes("-topmost", True)
        self._root.wm_attributes("-alpha", 0.85)
        self._root.configure(bg=self.BG_COLOR)
        self._root.geometry(
            f"{self._width}x{self.WINDOW_HEIGHT}+{self._x}+{self._y}"
        )

        # â”€â”€ å·¥å…·åˆ— â”€â”€
        toolbar = tk.Frame(self._root, bg=self.TOOLBAR_BG, height=self.TOOLBAR_HEIGHT)
        toolbar.pack(fill="x", side="top")

        self._dir_btn_var = tk.StringVar(value="[ENâ†’ZH â‡„]")
        tk.Button(
            toolbar,
            textvariable=self._dir_btn_var,
            font=("Arial", 10),
            fg=self.BTN_COLOR,
            bg=self.BTN_BG,
            relief="flat",
            padx=8,
            command=self._toggle_direction,
        ).pack(side="left", padx=4, pady=2)

        self._src_btn_var = tk.StringVar(value="[ğŸ”Š MON]")
        tk.Button(
            toolbar,
            textvariable=self._src_btn_var,
            font=("Arial", 10),
            fg=self.BTN_COLOR,
            bg=self.BTN_BG,
            relief="flat",
            padx=8,
            command=self._switch_source,
        ).pack(side="left", padx=4, pady=2)

        tk.Button(
            toolbar,
            text="âœ•",
            font=("Arial", 10),
            fg=self.BTN_COLOR,
            bg=self.BTN_BG,
            relief="flat",
            padx=8,
            command=self._do_close,
        ).pack(side="right", padx=4, pady=2)

        # è‹±æ–‡è¡Œ
        self._en_var = tk.StringVar()
        tk.Label(
            self._root,
            textvariable=self._en_var,
            font=self.EN_FONT,
            fg=self.EN_COLOR,
            bg=self.BG_COLOR,
            anchor="w",
            padx=20,
        ).pack(fill="x", pady=(10, 0))

        # ä¸­æ–‡è¡Œ
        self._zh_var = tk.StringVar()
        tk.Label(
            self._root,
            textvariable=self._zh_var,
            font=self.ZH_FONT,
            fg=self.ZH_COLOR,
            bg=self.BG_COLOR,
            anchor="w",
            padx=20,
        ).pack(fill="x")

        self._root.bind("<Escape>", lambda e: self._do_close())
        self._root.bind("<F9>", lambda e: self._toggle_direction())
        self._root.protocol("WM_DELETE_WINDOW", self._do_close)

    def _do_close(self):
        """é—œé–‰è¦–çª—ã€‚"""
        self._root.destroy()

    def _toggle_direction(self):
        if self._on_toggle_direction:
            new_dir = self._on_toggle_direction()
            self.update_direction_label(new_dir)

    def update_direction_label(self, direction: str):
        label = f"[{direction} â‡„]"
        self._root.after(0, lambda: self._dir_btn_var.set(label))

    def _switch_source(self):
        if self._on_switch_source:
            self._on_switch_source()

    def update_source_label(self, source: str):
        label = "[ğŸ¤ MIC]" if source == "mic" else "[ğŸ”Š MON]"
        self._root.after(0, lambda: self._src_btn_var.set(label))

    def set_text(self, original: str = "", translated: str = ""):
        """å¾ä»»æ„åŸ·è¡Œç·’å®‰å…¨åœ°æ›´æ–°å­—å¹•ï¼ˆç”¨ after() æ’ç¨‹åˆ°ä¸»åŸ·è¡Œç·’ï¼‰ã€‚"""
        def _update():
            self._en_var.set(original[-120:] if len(original) > 120 else original)
            self._zh_var.set(translated[-60:] if len(translated) > 60 else translated)
        self._root.after(0, _update)

    def run(self):
        """å•Ÿå‹• tkinter mainloopï¼ˆé˜»å¡ï¼Œå¿…é ˆåœ¨ä¸»åŸ·è¡Œç·’å‘¼å«ï¼‰ã€‚"""
        self._root.mainloop()

# ---------------------------------------------------------------------------
# Audio Sources
# ---------------------------------------------------------------------------

TARGET_SR = 16000
CHUNK_SAMPLES = 8000  # 0.5 ç§’ @ 16kHz


class AudioSource(ABC):
    """éŸ³è¨Šä¾†æºæŠ½è±¡ä»‹é¢ã€‚æœªä¾†å¯æ–°å¢ MicrophoneAudioSourceã€NetworkAudioSource ç­‰ã€‚"""

    @abstractmethod
    def start(self, callback: Callable[[np.ndarray], None]) -> None:
        """é–‹å§‹æ“·å–éŸ³è¨Šï¼Œæ¯ 0.5 ç§’ä»¥ 16kHz float32 mono ndarray å‘¼å« callbackã€‚"""

    @abstractmethod
    def stop(self) -> None:
        """åœæ­¢æ“·å–ã€‚"""

    @staticmethod
    def list_devices() -> None:
        """åˆ—å‡ºç³»çµ±éŸ³è¨Šè£ç½®åŠ PulseAudio monitor sourcesã€‚"""
        import sounddevice as sd
        print("=== ALSA è£ç½®æ¸…å–® ===")
        print(sd.query_devices())
        print("\n=== PulseAudio Monitor Sourcesï¼ˆå¯ç”¨æ–¼ --monitor-deviceï¼‰===")
        try:
            result = subprocess.run(
                ["pactl", "list", "sources", "short"],
                capture_output=True, text=True, timeout=3,
            )
            for line in result.stdout.splitlines():
                if "monitor" in line.lower():
                    print(" ", line)
        except Exception:
            print("  ï¼ˆç„¡æ³•å–å¾— PulseAudio sourcesï¼Œè«‹ç¢ºèª pactl å·²å®‰è£ï¼‰")


class MonitorAudioSource(AudioSource):
    """
    æ“·å– PipeWire/PulseAudio monitor sourceï¼ˆç³»çµ±æ’­æ”¾éŸ³è¨Šï¼‰ã€‚

    ä½¿ç”¨ queue.Queue è§£è€¦éŸ³è¨Š callback èˆ‡ ASR HTTP è«‹æ±‚ï¼Œé¿å…
    é˜»å¡æ“ä½œæ±¡æŸ“å³æ™‚éŸ³è¨ŠåŸ·è¡Œç·’ã€‚

    é€é ALSA pulse è¨­å‚™ + PULSE_SOURCE ç’°å¢ƒè®Šæ•¸é¸æ“‡ monitor sourceï¼Œ
    è®“ sounddevice èƒ½å­˜å– PipeWire/PulseAudio monitorã€‚

    device é è¨­ï¼šalsa_output.pci-0000_00_1f.3.iec958-stereo.monitor
    """

    DEFAULT_DEVICE = "alsa_output.pci-0000_00_1f.3.iec958-stereo.monitor"
    ALSA_PULSE_DEVICE = "pulse"  # ALSA pulse pluginï¼Œé€éå®ƒå­˜å– PulseAudio

    def __init__(self, device: str | None = None):
        self._device = device or self.DEFAULT_DEVICE  # PulseAudio source åç¨±
        self._stream = None
        self._buf: np.ndarray = np.zeros(0, dtype=np.float32)
        self._native_sr: int = 0
        self._callback: Callable[[np.ndarray], None] | None = None
        self._queue: queue.Queue = queue.Queue()
        self._running: bool = False
        self._consumer_thread: threading.Thread | None = None

    def start(self, callback: Callable[[np.ndarray], None]) -> None:
        if self._stream is not None:
            raise RuntimeError("MonitorAudioSource is already running; call stop() first.")

        import sounddevice as sd

        # è¨­å®š PULSE_SOURCE è®“ PulseAudio ä½¿ç”¨æŒ‡å®šçš„ monitor source
        os.environ["PULSE_SOURCE"] = self._device

        # é€é ALSA pulse è¨­å‚™å–å¾— native samplerate
        dev_info = sd.query_devices(self.ALSA_PULSE_DEVICE, kind="input")
        self._native_sr = int(dev_info["default_samplerate"])  # é€šå¸¸ 44100 æˆ– 48000
        self._callback = callback
        self._buf = np.zeros(0, dtype=np.float32)
        self._running = True

        # æ¶ˆè²»è€…åŸ·è¡Œç·’ï¼šå¾ queue å–éŸ³è¨Šã€resampleã€é€ callback
        self._consumer_thread = threading.Thread(target=self._consumer, daemon=True)
        self._consumer_thread.start()

        # éŸ³è¨Š streamï¼šcallback åªåš enqueueï¼ˆä¸é˜»å¡ï¼‰
        self._stream = sd.InputStream(
            samplerate=self._native_sr,
            channels=1,
            dtype="float32",
            blocksize=int(self._native_sr * 0.05),  # 50ms å›ºå®š buffer
            device=self.ALSA_PULSE_DEVICE,
            callback=self._sd_callback,
        )
        self._stream.start()

    def _sd_callback(self, indata: np.ndarray, frames: int, time_info, status) -> None:
        """éŸ³è¨ŠåŸ·è¡Œç·’ callbackï¼šåªåšæœ€è¼•é‡çš„ enqueueï¼Œä¸åšä»»ä½•é˜»å¡æ“ä½œã€‚"""
        if status:
            print(f"[Audio] {status}")
        self._queue.put(indata[:, 0].copy())

    def _consumer(self) -> None:
        """æ¶ˆè²»è€…åŸ·è¡Œç·’ï¼šresample + ç´¯ç© buffer + å‘¼å« ASR callbackã€‚"""
        while self._running:
            try:
                raw = self._queue.get(timeout=0.1)
            except queue.Empty:
                continue

            # resample native_sr â†’ 16kHzï¼ˆåœ¨éå³æ™‚åŸ·è¡Œç·’ä¸­é€²è¡Œï¼‰
            target_len = int(len(raw) * TARGET_SR / self._native_sr)
            resampled = signal.resample(raw, target_len).astype(np.float32)
            self._buf = np.concatenate([self._buf, resampled])

            # æ¯ç´¯ç© CHUNK_SAMPLES å°±é€å‡ºä¸€æ¬¡
            while len(self._buf) >= CHUNK_SAMPLES:
                chunk = self._buf[:CHUNK_SAMPLES].copy()
                self._buf = self._buf[CHUNK_SAMPLES:]
                if self._callback:
                    self._callback(chunk)

    def stop(self) -> None:
        self._running = False
        if self._stream:
            self._stream.stop()
            self._stream.close()
            self._stream = None
        if self._consumer_thread:
            self._consumer_thread.join(timeout=1.0)
            self._consumer_thread = None
        self._buf = np.zeros(0, dtype=np.float32)


class MicrophoneAudioSource(AudioSource):
    """éº¥å…‹é¢¨éŸ³è¨Šä¾†æºã€‚"""

    def __init__(self, device=None):
        self._device = device  # None = ç³»çµ±é è¨­éº¥å…‹é¢¨
        self._stream = None
        self._buf: np.ndarray = np.zeros(0, dtype=np.float32)
        self._native_sr: int = 0
        self._callback: Callable[[np.ndarray], None] | None = None
        self._queue: queue.Queue = queue.Queue()
        self._running: bool = False
        self._consumer_thread: threading.Thread | None = None

    def start(self, callback: Callable[[np.ndarray], None]) -> None:
        if self._stream is not None:
            raise RuntimeError("MicrophoneAudioSource is already running; call stop() first.")
        import sounddevice as sd
        dev_info = sd.query_devices(self._device, kind="input")
        self._native_sr = int(dev_info["default_samplerate"])
        self._callback = callback
        self._buf = np.zeros(0, dtype=np.float32)
        self._running = True
        self._consumer_thread = threading.Thread(target=self._consumer, daemon=True)
        self._consumer_thread.start()
        self._stream = sd.InputStream(
            samplerate=self._native_sr,
            channels=1,
            dtype="float32",
            blocksize=int(self._native_sr * 0.05),
            device=self._device,
            callback=self._sd_callback,
        )
        self._stream.start()

    def _sd_callback(self, indata: np.ndarray, frames: int, time_info, status) -> None:
        if status:
            print(f"[Audio] {status}")
        self._queue.put(indata[:, 0].copy())

    def _consumer(self) -> None:
        while self._running:
            try:
                raw = self._queue.get(timeout=0.1)
            except queue.Empty:
                continue
            target_len = int(len(raw) * TARGET_SR / self._native_sr)
            resampled = signal.resample(raw, target_len).astype(np.float32)
            self._buf = np.concatenate([self._buf, resampled])
            while len(self._buf) >= CHUNK_SAMPLES:
                chunk = self._buf[:CHUNK_SAMPLES].copy()
                self._buf = self._buf[CHUNK_SAMPLES:]
                if self._callback:
                    self._callback(chunk)

    def stop(self) -> None:
        self._running = False
        if self._stream:
            self._stream.stop()
            self._stream.close()
            self._stream = None
        if self._consumer_thread:
            self._consumer_thread.join(timeout=1.0)
            self._consumer_thread = None
        self._buf = np.zeros(0, dtype=np.float32)

# ---------------------------------------------------------------------------
# Worker Processï¼ˆéŸ³è¨Š + ASR + ç¿»è­¯ï¼Œç„¡ X11ï¼‰
# ---------------------------------------------------------------------------

def _worker_main(text_q: multiprocessing.SimpleQueue, cmd_q: multiprocessing.SimpleQueue, cfg: dict) -> None:
    """
    åœ¨ç¨ç«‹ subprocess åŸ·è¡Œï¼šsounddevice + VAD + ASR + ç¿»è­¯ã€‚
    å®Œå…¨ä¸ä½¿ç”¨ X11/tkinterï¼Œé¿å…èˆ‡ä¸»ç¨‹åºçš„ XCB è¡çªã€‚

    text_q: é€å‡º {"original": str, "translated": str} æˆ– {"direction": str}
    cmd_q:  æ¥æ”¶ "toggle"ï¼ˆåˆ‡æ›ç¿»è­¯æ–¹å‘ï¼‰æˆ– "stop"

    æ¶æ§‹ï¼š
    - on_chunkï¼šéé˜»å¡ï¼ŒåªæŠŠéŸ³è¨Šæ”¾å…¥ _vad_q
    - vad_loopï¼šSilero VAD åµæ¸¬èªéŸ³/éœéŸ³ï¼Œç´¯ç©èªéŸ³ç‰‡æ®µï¼Œ
                éœéŸ³ ~0.8s å¾ŒæŠŠå®Œæ•´èªéŸ³æ”¾å…¥ _speech_q
    - asr_loopï¼šç­‰å¾… _speech_qï¼Œé€åˆ° ASR serverï¼Œæ›´æ–°å­—å¹•
    """
    import onnxruntime as ort
    from pathlib import Path
    import opencc

    os.environ.pop("DISPLAY", None)

    # ç°¡é«”â†’å°ç£ç¹é«”è½‰æ›å™¨ï¼ˆs2twp åŒ…å«è©å½™æ›¿æ›ï¼Œå¦‚ã€Œè»Ÿä»¶â†’è»Ÿé«”ã€ï¼‰
    _s2tw = opencc.OpenCC("s2twp")

    current_original = ""

    def on_translation(translated: str) -> None:
        text_q.put({"original": current_original, "translated": translated})

    debouncer = TranslationDebouncer(
        api_key=cfg["openai_api_key"],
        callback=on_translation,
        model=cfg["translation_model"],
    )
    debouncer.set_direction(cfg["direction"])

    if cfg["source"] == "monitor":
        audio_source = MonitorAudioSource(device=cfg["monitor_device"])
    else:
        audio_source = MicrophoneAudioSource(device=cfg.get("mic_device"))

    asr = ASRClient(cfg["asr_server"])

    # Silero VAD å¸¸æ•¸ï¼ˆv6 æ¨¡å‹ï¼‰
    VAD_CHUNK = 576               # 36ms @ 16kHz
    VAD_THRESHOLD = 0.5
    RT_SILENCE_CHUNKS = 22        # 0.8s - çŸ­éœéŸ³ï¼šprobe å¥æœ«
    RT_LONG_SILENCE_CHUNKS = 55   # 2s   - é•·éœéŸ³ï¼šå¼·åˆ¶ flush
    RT_MAX_BUFFER_CHUNKS = 83     # 3s   - å¼·åˆ¶ flushï¼ˆé™åˆ¶å–®æ¬¡ push_chunk â‰¤ 3sï¼Œé¿å… server timeoutï¼‰

    # å¥æœ«ç¬¦è™Ÿï¼ˆASR å›å‚³è‹±æ–‡æ¨™é»æˆ–ä¸­æ–‡æ¨™é»çš†å¯ï¼‰
    SENTENCE_END_CHARS = frozenset('.?!ã€‚ï¼Ÿï¼â€¦')

    # è¼‰å…¥ VAD æ¨¡å‹
    _vad_model_path = Path(__file__).parent / "silero_vad_v6.onnx"
    vad_sess = ort.InferenceSession(str(_vad_model_path))

    _vad_q: queue.Queue = queue.Queue()
    # _speech_q å‚³é€ (audio: np.ndarray, event: str)
    # event = "probe" - çŸ­éœéŸ³ï¼Œæª¢æŸ¥æ˜¯å¦å¥æœ«å†æ±ºå®šè¦ä¸è¦é¡¯ç¤º
    # event = "force" - å¼·åˆ¶ flushï¼ˆé•·éœéŸ³æˆ– max bufferï¼‰
    _speech_q: queue.Queue = queue.Queue()
    _stop_event = threading.Event()

    def on_chunk(audio: np.ndarray) -> None:
        """éé˜»å¡ï¼šåªæŠŠéŸ³è¨Šæ”¾å…¥ VAD ä½‡åˆ—ã€‚"""
        _vad_q.put(audio)

    def vad_loop() -> None:
        """
        VAD åŸ·è¡Œç·’ï¼šå…©æ®µå¼éœéŸ³åµæ¸¬ã€‚

        çŸ­éœéŸ³ï¼ˆ0.8sï¼‰â†’ é€ (buf, "probe")ï¼Œç”± asr_loop æ±ºå®šæ˜¯å¦å¥æœ«
        é•·éœéŸ³ï¼ˆ2sï¼‰  â†’ é€ (empty, "force")ï¼Œå¼·åˆ¶é¡¯ç¤º
        max buffer   â†’ é€ (buf, "force")ï¼Œå¼·åˆ¶é¡¯ç¤º
        """
        h = np.zeros((1, 1, 128), dtype=np.float32)
        c = np.zeros((1, 1, 128), dtype=np.float32)
        buf: list[np.ndarray] = []
        sil_cnt = 0
        probed = False   # æ˜¯å¦å·²é€å‡º probeï¼ˆç­‰å¾… long silence æˆ–æ–°èªéŸ³ï¼‰
        leftover = np.zeros(0, dtype=np.float32)

        try:
            while not _stop_event.is_set():
                try:
                    audio = _vad_q.get(timeout=0.1)
                except queue.Empty:
                    continue

                audio = np.concatenate([leftover, audio])
                n_chunks = len(audio) // VAD_CHUNK
                leftover = audio[n_chunks * VAD_CHUNK:]

                for i in range(n_chunks):
                    chunk = audio[i * VAD_CHUNK:(i + 1) * VAD_CHUNK]
                    inp = chunk[np.newaxis, :].astype(np.float32)
                    out = vad_sess.run(
                        ["speech_probs", "hn", "cn"],
                        {"input": inp, "h": h, "c": c},
                    )
                    prob, h, c = out
                    prob = float(prob.flatten()[0])

                    if prob >= VAD_THRESHOLD:
                        buf.append(chunk)
                        sil_cnt = 0
                        probed = False
                    elif buf or sil_cnt > 0:
                        if buf:
                            buf.append(chunk)
                        sil_cnt += 1

                        if not probed and sil_cnt >= RT_SILENCE_CHUNKS:
                            # çŸ­éœéŸ³ï¼šé€ probeï¼Œæ¸…ç©º buf ä½†ä¿ç•™ session
                            probe_audio = np.concatenate(buf) if buf else np.zeros(0, dtype=np.float32)
                            _speech_q.put((probe_audio, "probe"))
                            buf = []
                            probed = True
                        elif probed and sil_cnt >= RT_LONG_SILENCE_CHUNKS:
                            # é•·éœéŸ³ï¼šå¼·åˆ¶ flush
                            _speech_q.put((np.zeros(0, dtype=np.float32), "force"))
                            sil_cnt = 0
                            probed = False
                            h = np.zeros((1, 1, 128), dtype=np.float32)
                            c = np.zeros((1, 1, 128), dtype=np.float32)

                    # Max bufferï¼šå¼·åˆ¶ flush
                    if len(buf) >= RT_MAX_BUFFER_CHUNKS:
                        _speech_q.put((np.concatenate(buf), "force"))
                        buf = []
                        sil_cnt = 0
                        probed = False
                        h = np.zeros((1, 1, 128), dtype=np.float32)
                        c = np.zeros((1, 1, 128), dtype=np.float32)

        except Exception as e:
            print(f"[VAD fatal error] {e}", flush=True)
            import traceback; traceback.print_exc()

    def _parse_asr_result(raw: str) -> tuple[str, str]:
        """å‰é™¤ 'language XXX<asr_text>' å‰ç¶´ï¼Œå›å‚³ (language, text)ã€‚"""
        language = ""
        if raw.startswith("language ") and "<asr_text>" in raw:
            header, text = raw.split("<asr_text>", 1)
            language = header.removeprefix("language ").strip()
            return language, text.strip()
        if "<asr_text>" in raw:
            return "", raw.split("<asr_text>", 1)[1].strip()
        return "", raw.strip()

    def _to_traditional(text: str, language: str) -> str:
        """è‹¥èªè¨€ç‚ºä¸­æ–‡ï¼Œå°‡ç°¡é«”è½‰æˆå°ç£ç¹é«”ã€‚"""
        if language and "chinese" in language.lower():
            return _s2tw.convert(text)
        return text

    def asr_loop() -> None:
        """
        ASR åŸ·è¡Œç·’ï¼šå¥å­çµ„åˆå™¨ï¼ˆsentence assemblerï¼‰ã€‚

        æ¯å€‹ 2s ç‰‡æ®µè¾¨è­˜å¾Œç´¯ç©åˆ° assembled_partsã€‚
        åªæœ‰ç•¶çµ„åˆå¾Œæ–‡å­—ä»¥å¥æœ«ç¬¦è™Ÿçµå°¾ï¼Œæˆ–å·²ç´¯ç© â‰¥ MAX_ASSEMBLE_PARTS å€‹ç‰‡æ®µæ™‚ï¼Œ
        æ‰é¡¯ç¤ºçµ„åˆå¾Œçš„å®Œæ•´å¥å­ã€‚

        probe  + å¥æœ«ç¬¦è™Ÿ  â†’ ç«‹å³é¡¯ç¤ºçµ„åˆçµæœ
        force              â†’ ç´¯ç©åˆ°çµ„åˆå™¨ï¼Œå¥æœ«æ‰é¡¯ç¤ºï¼ˆæˆ–ç‰‡æ®µæ•¸é”ä¸Šé™ï¼‰
        """
        nonlocal current_original
        assembled_parts: list[str] = []   # ç­‰å¾…çµ„åˆçš„ç‰‡æ®µ
        MAX_ASSEMBLE_PARTS = 3            # æœ€å¤šç´¯ç© ~6s çš„ç‰‡æ®µå†å¼·åˆ¶é¡¯ç¤º

        while not _stop_event.is_set():
            try:
                audio, event = _speech_q.get(timeout=0.5)
            except queue.Empty:
                continue

            if len(audio) < TARGET_SR // 8:   # < 0.125sï¼Œè·³é
                continue

            try:
                asr.start()
                result = asr.push_chunk(audio)
                inter_lang, intermediate_text = _parse_asr_result(result.get("text", ""))
                try:
                    fin = asr.finish()
                    fin_lang, fin_text = _parse_asr_result(fin.get("text", ""))
                    language = fin_lang or inter_lang
                    text = fin_text or intermediate_text
                except Exception:
                    language = inter_lang
                    text = intermediate_text

                text = _to_traditional(text, language)

                if not text:
                    continue

                assembled_parts.append(text)
                assembled = " ".join(assembled_parts)

                # åˆ¤æ–·æ˜¯å¦é¡¯ç¤ºï¼šå¥æœ«ç¬¦è™Ÿ OR é”åˆ°ç‰‡æ®µä¸Šé™
                sentence_done = assembled[-1] in SENTENCE_END_CHARS
                force_show = len(assembled_parts) >= MAX_ASSEMBLE_PARTS

                if sentence_done or force_show or event == "probe":
                    if assembled != current_original:
                        current_original = assembled
                        text_q.put({"original": assembled, "translated": ""})
                        # debouncer.update(assembled)  # ç¿»è­¯æš«æ™‚é—œé–‰
                    assembled_parts = []

            except Exception as e:
                print(f"[Worker ASR error] {e}", flush=True)
                try:
                    asr.finish()
                except Exception:
                    pass

    vad_thread = threading.Thread(target=vad_loop, daemon=True, name="vad-thread")
    asr_thread = threading.Thread(target=asr_loop, daemon=True, name="asr-thread")
    vad_thread.start()
    asr_thread.start()

    audio_source.start(on_chunk)
    print("[Worker] Audio capture started.", flush=True)

    try:
        while True:
            if not cmd_q.empty():
                cmd = cmd_q.get()
                if cmd == "toggle":
                    new_dir = debouncer.toggle_direction()
                    text_q.put({"direction": new_dir})
                elif cmd == "switch_source":
                    audio_source.stop()
                    if isinstance(audio_source, MonitorAudioSource):
                        audio_source = MicrophoneAudioSource(device=cfg.get("mic_device"))
                        src_name = "mic"
                    else:
                        audio_source = MonitorAudioSource(device=cfg["monitor_device"])
                        src_name = "monitor"
                    audio_source.start(on_chunk)
                    text_q.put({"source": src_name})
                elif cmd == "stop":
                    break
            else:
                time.sleep(0.1)
    finally:
        _stop_event.set()
        audio_source.stop()
        debouncer.shutdown()
        vad_thread.join(timeout=3)
        asr_thread.join(timeout=5)
        try:
            asr.finish()
        except Exception:
            pass
        print("[Worker] Stopped.", flush=True)


# ---------------------------------------------------------------------------
# Main Entry Point
# ---------------------------------------------------------------------------

def main() -> None:
    parser = argparse.ArgumentParser(description="Real-time subtitle overlay")
    parser.add_argument("--asr-server", default="http://localhost:8000",
                        help="Qwen3-ASR streaming server URL")
    parser.add_argument("--openai-api-key", default=os.environ.get("OPENAI_API_KEY", ""),
                        help="OpenAI API key (or set OPENAI_API_KEY env var)")
    parser.add_argument("--screen", type=int, default=0,
                        help="Display screen index (0=primary, 1=secondary)")
    parser.add_argument("--list-devices", action="store_true",
                        help="List available audio devices and exit")
    parser.add_argument("--translation-model", default="gpt-4o-mini",
                        help="OpenAI model for translation")
    parser.add_argument("--source", choices=["monitor", "mic"], default="monitor",
                        help="Audio source: monitorï¼ˆç³»çµ±éŸ³è¨Šï¼‰or micï¼ˆéº¥å…‹é¢¨ï¼‰")
    parser.add_argument("--monitor-device", default=MonitorAudioSource.DEFAULT_DEVICE,
                        help="PulseAudio monitor source nameï¼ˆç”¨ --list-devices æŸ¥è©¢ï¼‰")
    parser.add_argument("--mic-device", default=None,
                        help="éº¥å…‹é¢¨è£ç½®åç¨±æˆ–ç´¢å¼•ï¼ˆNone = ç³»çµ±é è¨­éº¥å…‹é¢¨ï¼‰")
    parser.add_argument("--direction", choices=["enâ†’zh", "zhâ†’en"], default="enâ†’zh",
                        help="Initial translation direction")
    args = parser.parse_args()

    if args.list_devices:
        AudioSource.list_devices()
        return

    if not args.openai_api_key:
        print("Error: --openai-api-key æˆ– OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸å¿…é ˆè¨­å®š")
        return

    cfg = {
        "asr_server": args.asr_server,
        "openai_api_key": args.openai_api_key,
        "translation_model": args.translation_model,
        "source": args.source,
        "monitor_device": args.monitor_device,
        "mic_device": args.mic_device,
        "direction": args.direction,
    }

    # æº–å‚™ IPC queuesï¼ˆç”¨ SimpleQueueï¼Œä¸æœƒåœ¨ä¸»ç¨‹åºç”¢ç”Ÿ feeder èƒŒæ™¯åŸ·è¡Œç·’ï¼‰
    text_q: multiprocessing.SimpleQueue = multiprocessing.SimpleQueue()
    cmd_q: multiprocessing.SimpleQueue = multiprocessing.SimpleQueue()

    # æœ¬åœ°æ–¹å‘è¿½è¹¤ï¼ˆUI ç”¨ï¼Œèˆ‡ worker åŒæ­¥ï¼‰
    current_direction = [args.direction]

    def on_toggle() -> str:
        current_direction[0] = "zhâ†’en" if current_direction[0] == "enâ†’zh" else "enâ†’zh"
        cmd_q.put("toggle")
        return current_direction[0]

    def on_switch_source() -> None:
        cmd_q.put("switch_source")

    # å…ˆå»ºç«‹ tkinterï¼ˆåœ¨ fork ä¹‹å‰å®Œæˆ X11 é€£ç·šï¼Œchild ç¹¼æ‰¿ fd ä½†ç«‹å³ç§»é™¤ DISPLAYï¼‰
    overlay = SubtitleOverlay(
        screen_index=args.screen,
        on_toggle_direction=on_toggle,
        on_switch_source=on_switch_source,
    )
    overlay.update_direction_label(args.direction)

    # tkinter åˆå§‹åŒ–å¾Œæ‰ fork workerï¼ˆchild ä¸ä½¿ç”¨ X11ï¼‰
    worker = multiprocessing.Process(
        target=_worker_main, args=(text_q, cmd_q, cfg),
        daemon=True, name="subtitle-worker",
    )
    worker.start()

    # ç”¨ tkinter after() è¼ªè©¢ text_qï¼ˆå…¨åœ¨ä¸»åŸ·è¡Œç·’ï¼Œé›¶ X11 ç«¶çˆ­ï¼‰
    def poll() -> None:
        while not text_q.empty():
            msg = text_q.get()
            if "direction" in msg:
                overlay.update_direction_label(msg["direction"])
            elif "source" in msg:
                overlay.update_source_label(msg["source"])
            else:
                overlay.set_text(
                    original=msg.get("original", ""),
                    translated=msg.get("translated", ""),
                )
        overlay._root.after(50, poll)

    overlay._root.after(50, poll)
    overlay.run()  # blockingï¼Œç›´åˆ°è¦–çª—é—œé–‰

    # è¦–çª—é—œé–‰å¾Œåœæ­¢ worker
    cmd_q.put("stop")
    worker.join(timeout=3)
    if worker.is_alive():
        worker.terminate()


if __name__ == "__main__":
    # spawnï¼šå…¨æ–° Python ç¨‹åºï¼Œä¸ç¹¼æ‰¿ X11 socket fdï¼Œé¿å… XCB åºè™Ÿè¡çª
    multiprocessing.set_start_method("spawn")
    main()
